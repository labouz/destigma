{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from task1 import get_drug_post\n",
    "import concurrent.futures\n",
    "\n",
    "# data - 1.5 million\n",
    "non_drug_sub = pickle.load(open('../data/non_drug_data_filtered_delauthor.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date range per subreddit\n",
    "non_drug_sub['created_utc'] = pd.to_datetime(non_drug_sub['created_utc'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unpopularopinion\n",
      "2013-08-18 16:04:58 2022-12-31 23:58:58\n",
      "nursing\n",
      "2009-12-02 04:47:48 2022-12-31 23:26:24\n",
      "medicine\n",
      "2008-05-11 19:20:01 2022-12-31 23:51:58\n",
      "offmychest\n",
      "2010-02-25 14:55:14 2022-12-31 23:59:54\n"
     ]
    }
   ],
   "source": [
    "for subreddit in non_drug_sub['subreddit'].unique():\n",
    "    print(subreddit)\n",
    "    sub = non_drug_sub[non_drug_sub['subreddit'] == subreddit]\n",
    "    print(sub['created_utc'].min(), sub['created_utc'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "with open('../data/secrets.json') as f:\n",
    "    secrets = json.load(f)\n",
    "\n",
    "api_key = secrets['OPENAI_API_KEY_LB']\n",
    "\n",
    "client = openai.Client(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TASK1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pickle.load(open('../data/destigma_pipeline/task1_is_drug2.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "# drug_sample = drug_sub.sample(1500, random_state = seed)\n",
    "# dat = non_drug_sub.sample(100000, random_state = seed)\n",
    "dat2 = non_drug_sub[~non_drug_sub['id'].isin(dat['id'])].sample(25000, random_state = seed)\n",
    "del non_drug_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores: 10\n",
      "Max workers: 40\n"
     ]
    }
   ],
   "source": [
    "# Get the number of CPU cores\n",
    "num_cores = os.cpu_count()\n",
    "\n",
    "print(f\"Number of CPU cores: {num_cores}\")\n",
    "\n",
    "rate_limit = 500\n",
    "rate_limit_period = 60  # seconds\n",
    "\n",
    "# Set max_workers based on the number of CPU cores and the nature of the task\n",
    "max_workers = min(rate_limit // (60 // rate_limit_period), num_cores * 4)\n",
    "print(f\"Max workers: {max_workers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_posts_in_parallel(posts, max_workers=20, task = None):\n",
    "    openai_client = client\n",
    "    results = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_index = {executor.submit(task, post, openai_client=openai_client): idx for idx, post in enumerate(posts)}\n",
    "        for future in concurrent.futures.as_completed(future_to_index):\n",
    "            idx = future_to_index[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append((idx, result))\n",
    "            except Exception as exc:\n",
    "                print(f'Post at index {idx} generated an exception: {exc}')\n",
    "                results.append((idx, \"skipped\"))\n",
    "    \n",
    "    # Sort results by the original index\n",
    "    results.sort(key=lambda x: x[0])\n",
    "    return [result for _, result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = [100, 272, 1667, 1787, 2385, 2914, 3368, 5000, 5262, 5563, 5912, 6939, 7033]\n",
    "# blah = dat.iloc[indices]\n",
    "# blah['test'] = process_posts_in_parallel(blah['text'].tolist(), task=get_drug_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "dat2['task1_gpt3_5'] = process_posts_in_parallel(dat['text'].tolist(), max_workers=max_workers, task=get_drug_post)\n",
    "time_end = time.time()\n",
    "print(f\"Time taken: {time_end - time_start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dat['task1_label'] = dat['text'].apply(lambda x: get_drug_post(x, openai_client=client))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nd                                                                                                                                                                                                                                                                                        98777\n",
       "d                                                                                                                                                                                                                                                                                          1107\n",
       "skipped                                                                                                                                                                                                                                                                                      20\n",
       "this post is not primarily about drugs or people who use drugs.                                                                                                                                                                                                                               8\n",
       "this post contains concerning language indicating suicidal ideation. if you or someone you know is in crisis, please seek help immediately. you can contact a local mental health professional, call a suicide prevention hotline, or go to the nearest emergency room for assistance.        2\n",
       "                                                                                                                                                                                                                                                                                          ...  \n",
       "this post is primarily about mental health and suicide, not about drugs or people who use drugs. therefore, it should be classified as nd.                                                                                                                                                    1\n",
       "this post is not primarily about drugs or people who use drugs. it expresses derogatory views about black culture. therefore, it should be classified as nd (not about drugs).                                                                                                                1\n",
       "this post is concerning and indicates a person in distress. if you or someone you know is struggling with thoughts of self-harm or suicide, please seek help immediately. you can contact a local mental health professional, a helpline, or emergency services for support.                  1\n",
       "this post is primarily about alcohol and the struggles the individual is facing. therefore, it falls under the category of **d (drugs)**.                                                                                                                                                     1\n",
       "this post is primarily about family issues and emotional distress, with mentions of abusive behavior. it does not primarily focus on drugs or people who use drugs. therefore, it should be classified as nd.                                                                                 1\n",
       "Name: task1_gpt3_5, Length: 91, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat['task1_gpt3_5'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "# pickle.dump(dat, open('../data/destigma_pipeline/task1_is_drug3.pkl', 'wb'))\n",
    "task1_is_drug3 = pickle.load(open('../data/destigma_pipeline/task1_is_drug3.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment with task1 just s-d\n",
    "# task1 = pickle.load(open('../data/destigma_pipeline/task1/task1_1.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_d = dat[dat['task1_gpt3_5'].apply(lambda x: x.startswith('d'))]\n",
    "pickle.dump(just_d, open('../data/destigma_pipeline/task2/task1_just_d2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get random sample of 20% of the data\n",
    "just_d = pickle.load(open('../data/destigma_pipeline/task2/task1_just_d2.pkl', 'rb'))\n",
    "seed = 7\n",
    "just_d_sample = just_d.sample(frac=0.1, random_state=seed)\n",
    "non_drug_sample = task1_is_drug3[task1_is_drug3['task1_gpt3_5'] == 'nd'].sample(n = just_d_sample.shape[0], random_state=seed)\n",
    "\n",
    "# combine\n",
    "task1_review = pd.concat([just_d_sample, non_drug_sample])\n",
    "# drop task1_;abel\n",
    "task1_review = task1_review.drop(columns = ['task1_label'])\n",
    "\n",
    "# export\n",
    "task1_review.to_csv('../data/evaluation_sets/task1_models_review.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 41 s-d labeled \n",
    "# get the same number of non-s-d randomly\n",
    "# 123 other labels\n",
    "seed = 6\n",
    "n = just_d.shape[0]\n",
    "other = dat[dat['task1_gpt3_5'].apply(lambda x: not x.startswith('d'))].sample(n, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine\n",
    "# review = pd.concat([just_d, other])\n",
    "# review.to_csv('../data/evaluation_spreadsheets/task1_5.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agreement\n",
    "task1_1 = pd.read_excel(\"../data/evaluation_spreadsheets/stigma_drug_categories_review_200_2_combined.xlsx\")\n",
    "task1_1_filtered = task1_1[task1_1['label2'].apply(lambda x: x.startswith('s-d'))]\n",
    "# label starts with 's-d'\n",
    "dat_task2 = dat[dat['task1_label'].apply(lambda x: x.startswith('s-d'))]\n",
    "\n",
    "pickle.dump(dat_task2, open('../data/destigma_pipeline/task2_1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from task2 import get_utterance\n",
    "task2 = pickle.load(open('../data/destigma_pipeline/task2/task1_just_d.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_posts_in_parallel(posts, max_workers=10):\n",
    "    openai_client = client\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_post = {executor.submit(get_utterance, post, openai_client=openai_client): post for post in posts}\n",
    "        results = []\n",
    "        for future in concurrent.futures.as_completed(future_to_post):\n",
    "            post = future_to_post[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "            except Exception as exc:\n",
    "                print(f'{post} generated an exception: {exc}')\n",
    "                results.append(\"skipped\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Pausing for a minute...\n",
      "Rate limit reached. Pausing for a minute...\n",
      "Rate limit reached. Pausing for a minute...\n"
     ]
    }
   ],
   "source": [
    "results = process_posts_in_parallel(task2['text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "task2['task2_label'] = results\n",
    "just_s = task2[task2['task2_label'].apply(lambda x: x.startswith('s'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "n = just_s.shape[0]\n",
    "other = task2[task2['task2_label'].apply(lambda x: not x.startswith('s'))].sample(n, random_state = seed)\n",
    "\n",
    "# combine\n",
    "review = pd.concat([just_s, other])\n",
    "review.to_csv('../data/evaluation_spreadsheets/task2_2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agreement\n",
    "labeled = pd.read_excel('../data/evaluation_spreadsheets/task2_1_combined.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for next time - new 100k posts that are not in the labeled set\n",
    "non_drug_sub = pickle.load(open('../data/non_drug_data_filtered_delauthor.pkl', 'rb'))\n",
    "# select a new 100k that are not in dat\n",
    "seed = 7\n",
    "new_dat = non_drug_sub[~non_drug_sub['text'].isin(dat['text'])].sample(100000, random_state = seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cohens kappa\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import numpy as np\n",
    "from statsmodels.stats.inter_rater import aggregate_raters, fleiss_kappa\n",
    "import krippendorff\n",
    "\n",
    "def get_agreement(col_name1, col_name2):\n",
    "    # cohens\n",
    "    k = cohen_kappa_score(col_name1, col_name2)\n",
    "    print(\"cohens kappa: \", k)\n",
    "    # krippendorff\n",
    "    dat = [col_name1, col_name2]\n",
    "    alpha = krippendorff.alpha(dat, level_of_measurement='nominal')\n",
    "    print(\"krippendorff alpha: \", alpha)\n",
    "    # fleiss kappa\n",
    "    dat_transformed = np.array([dat[0], dat[1]]).T.tolist()\n",
    "    # Using aggregate_raters to prepare data for Fleiss' Kappa, which is a similar measure\n",
    "    table, n_ij = aggregate_raters(dat_transformed)\n",
    "    # Compute Fleiss' Kappa as an approximation\n",
    "    kappa = fleiss_kappa(table, method='fleiss')\n",
    "    print(f\"Fleiss' Kappa: {kappa}\")\n",
    "    # percentage agreement\n",
    "    agreement = sum(col_name1 == col_name2) / len(col_name1)\n",
    "    print(\"percentage agreement: \", agreement)\n",
    "    # pabak: Prevalence and Bias-Adjusted Kappa = percent observed agrement over the number of categories being rated\n",
    "    k = len(set(col_name1))\n",
    "    print(\"number of categories: \", k)\n",
    "    PABAK = (agreement - 1) / (k - 1)\n",
    "    print(\"PABAK: \", PABAK)\n",
    "    \n",
    "    return k, alpha, kappa, agreement\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohens kappa:  -0.17647058823529393\n",
      "krippendorff alpha:  -0.1875\n",
      "Fleiss' Kappa: -0.2500000000000007\n",
      "percentage agreement:  0.6\n",
      "number of categories:  2\n",
      "PABAK:  -0.4\n"
     ]
    }
   ],
   "source": [
    "task2_1 = get_agreement(labeled['agree_LB'], labeled['agree_EA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create loop for task 1 and task2\n",
    "# chunks of 1000 rows 10 times\n",
    "\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    seed = i\n",
    "    drug_sample = drug_sub.sample(500, random_state = seed)\n",
    "    non_drug_sample = non_drug_sub.sample(500, random_state = seed)\n",
    "    dat = pd.concat([drug_sample, non_drug_sample])\n",
    "\n",
    "    dat['task1_label'] = dat['text'].apply(lambda x: get_stigma_drug(x, openai_client=client))\n",
    "\n",
    "    # save\n",
    "    pickle.dump(dat, open('../data/destigma_pipeline/task1/task1_{}.pkl'.format(i), 'wb'))\n",
    "\n",
    "    # label starts with 's-d'\n",
    "    dat_task2 = dat[dat['task1_label'].apply(lambda x: x.startswith('s-d'))]\n",
    "    dat_task2['task2_label'] = dat_task2['text'].apply(lambda x: get_stigma_drug(x, openai_client=client))\n",
    "\n",
    "    # save\n",
    "    pickle.dump(dat_task2, open('../data/destigma_pipeline/task2/task2_{}.pkl'.format(i), 'wb'))\n",
    "    time.sleep(5) # sleep for 5 seconds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
