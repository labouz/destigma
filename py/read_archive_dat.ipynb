{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get data\n",
    "- get similar posts\n",
    "- create prompt with gpt4\n",
    "- tweak prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from process_zst2 import process_file\n",
    "\n",
    "dat_path = '/Users/laylabouzoubaa/Projects/PHD_RESEARCH/theeye/data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subreddits1 = ['ambien', 'benzodiazepines', 'cannabis', 'cocaine', 'LSD', 'MDMA', 'opiates',\n",
    "        #  'Psychedelics', 'shrooms',  'meth', 'ketamine', 'mescaline', 'kratom', 'fentanyl', 'DMT', 'adderall']\n",
    "\n",
    "# subreddits2 = ['leaves', 'fentanyl', 'DMT', 'adderall']\n",
    "# skipping trees and stims since they cause problems, need to come back to them later\n",
    "\n",
    "# 4/29 after selecting subreddits more systematically, decided to take the top subreddit from each class\n",
    "subreddits3 = ['benzodiazepines', 'Drugs', 'LSD','opiates', 'cocaine', 'kratom']\n",
    "# 4/29 trees makes python in VSC hang so i read the zat separately in colab and appended it to the rest of the data\n",
    "# subreddits3 = ['trees']\n",
    "submission_vars = ['author', 'created_utc', 'subreddit', 'title', 'selftext', 'num_comments', 'score', 'id']\n",
    "\n",
    "# adding some non-drug subreddits to the mix 05/1\n",
    "non_drug_subreddits = ['nursing', 'medicine', 'unpopularopinion','offmychest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data\n",
    "import pickle\n",
    "with open('../data/all_data_20240429.pkl', 'rb') as f:\n",
    "    all_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trees\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2011-02-02 09:06:02 : 100,000 : 0 : 29,360,800:4%\n",
      "2011-06-27 15:52:26 : 200,000 : 0 : 45,483,025:6%\n",
      "2011-09-27 13:53:20 : 300,000 : 0 : 61,867,400:9%\n",
      "2011-12-03 03:15:41 : 400,000 : 0 : 61,867,400:9%\n",
      "2012-02-02 16:50:59 : 500,000 : 0 : 78,645,000:11%\n",
      "2012-03-30 20:26:00 : 600,000 : 0 : 95,422,600:14%\n",
      "2012-05-17 05:32:00 : 700,000 : 0 : 112,200,200:16%\n",
      "2012-07-12 20:29:31 : 800,000 : 0 : 129,371,025:18%\n",
      "2012-09-06 00:20:12 : 900,000 : 0 : 147,197,225:21%\n",
      "2012-11-20 03:54:29 : 1,000,000 : 0 : 165,023,425:23%\n",
      "2013-01-26 01:58:17 : 1,100,000 : 0 : 165,023,425:23%\n",
      "2013-04-07 06:31:55 : 1,200,000 : 0 : 182,849,625:26%\n",
      "2013-06-29 06:56:42 : 1,300,000 : 0 : 200,675,825:28%\n",
      "2013-10-18 15:32:16 : 1,400,000 : 0 : 218,764,175:31%\n",
      "2014-02-01 06:34:56 : 1,500,000 : 0 : 237,638,975:34%\n",
      "2014-05-15 08:14:04 : 1,600,000 : 0 : 257,169,150:37%\n",
      "2014-09-18 23:31:02 : 1,700,000 : 0 : 277,354,700:39%\n",
      "2015-01-09 21:23:44 : 1,800,000 : 0 : 298,457,775:42%\n",
      "2015-05-04 07:33:43 : 1,900,000 : 0 : 321,264,825:46%\n",
      "2015-09-28 22:04:40 : 2,000,000 : 0 : 343,416,500:49%\n",
      "2016-03-20 07:12:59 : 2,100,000 : 0 : 364,912,800:52%\n",
      "2016-09-24 02:21:31 : 2,200,000 : 0 : 386,802,325:55%\n",
      "2017-02-21 03:45:33 : 2,300,000 : 0 : 429,532,775:61%\n",
      "2017-07-26 06:12:01 : 2,400,000 : 0 : 447,621,125:64%\n",
      "2018-02-01 17:46:37 : 2,500,000 : 0 : 464,923,025:66%\n",
      "2018-07-26 05:31:15 : 2,600,000 : 0 : 493,104,150:70%\n",
      "2019-01-21 04:07:32 : 2,700,000 : 0 : 520,236,675:74%\n",
      "2019-07-09 19:08:44 : 2,800,000 : 0 : 547,238,125:78%\n",
      "2020-01-25 05:54:54 : 2,900,000 : 0 : 573,453,125:81%\n",
      "2020-07-26 00:54:53 : 3,000,000 : 0 : 599,537,050:85%\n",
      "2021-04-01 19:23:00 : 3,100,000 : 0 : 634,796,225:90%\n",
      "2022-01-25 15:42:28 : 3,200,000 : 0 : 671,104,000:95%\n",
      "2022-10-29 14:16:24 : 3,300,000 : 0 : 704,369,366:100%\n"
     ]
    }
   ],
   "source": [
    "# reading takes approc 12 minutes\n",
    "all_data = pd.DataFrame(columns=submission_vars)\n",
    "\n",
    "for subreddit in subreddits3:\n",
    "    print(subreddit)\n",
    "    file = process_file(dat_path + subreddit + '/' + subreddit + '_submissions.zst', field=\"subreddit\", value = subreddit)\n",
    "    file = file[submission_vars]\n",
    "    all_data = pd.concat([all_data, file], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_non_drug = pd.DataFrame(columns=submission_vars)\n",
    "for subreddit in non_drug_subreddits:\n",
    "    print(subreddit)\n",
    "    file = process_file(dat_path + subreddit + '/' + subreddit + '_submissions.zst', field=\"subreddit\", value = subreddit)\n",
    "    file = file[submission_vars]\n",
    "    all_data_non_drug = pd.concat([all_data_non_drug, file], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nursing     212755\n",
      "medicine    116702\n",
      "Name: subreddit, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(all_data_non_drug.subreddit.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_non_drug.to_pickle('../data/nursing_medicine_subs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data2 = pd.DataFrame(columns=submission_vars)\n",
    "# for subreddit in subreddits2:\n",
    "#     print(subreddit)\n",
    "#     file = process_file(dat_path + subreddit + '/' + subreddit + '_submissions.zst', field=\"subreddit\", value = subreddit)\n",
    "#     file = file[submission_vars]\n",
    "#     all_data2 = pd.concat([all_data2, file], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all data - 2.5 million records\n",
    "# pickle is 827 MB!\n",
    "# import pickle\n",
    "# with open('../data/all_data_20240429.pkl', 'wb') as f:\n",
    "#     pickle.dump(all_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Skipping subreddits2 list. trees causes python to hang.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data appended in Google Colab BC of TREES subreddit\n",
    "import pickle\n",
    "# all_data = pickle.load(open('../data/all_data.pkl', 'rb'))\n",
    "unpopular = pickle.load(open('../data/unpopularopinion.pkl', 'rb'))\n",
    "nursing = pickle.load(open('../data/nursing_medicine_subs.pkl', 'rb'))\n",
    "offmychest = pickle.load(open('../data/offmychest.pkl', 'rb'))\n",
    "\n",
    "all_data = pd.concat([unpopular, nursing, offmychest], axis=0)\n",
    "# total 3.8 million records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del unpopular, nursing, offmychest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter dat\n",
    "\n",
    "# remove any rows with removed selftext or deleted or text is less than 10 words\n",
    "all_data = all_data[all_data['selftext'] != '[removed]']\n",
    "all_data = all_data[all_data['selftext'] != '[deleted]']\n",
    "\n",
    "# concatenate title and selftext\n",
    "all_data['text'] = all_data['title'] + ' ' + all_data['selftext']\n",
    "all_data = all_data[all_data['text'].str.split().str.len() > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filtered data\n",
    "with open('../data/non_drug_data_filtered.pkl', 'wb') as f:\n",
    "    pickle.dump(all_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "non_drug_data_filtered = pickle.load(open('../data/non_drug_data_filtered.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter deleted author\n",
    "non_drug_data_filtered = non_drug_data_filtered[non_drug_data_filtered['author'] != '[deleted]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/non_drug_data_filtered_delauthor.pkl', 'wb') as f:\n",
    "    pickle.dump(non_drug_data_filtered, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_filtered = pickle.load(open('../data/all_data_filtered.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trees              1564991\n",
       "Drugs               455130\n",
       "LSD                 246402\n",
       "opiates             186297\n",
       "cocaine             109957\n",
       "kratom               96469\n",
       "benzodiazepines      75243\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_filtered[\"subreddit\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_filtered = all_data_filtered[all_data_filtered['author'] != '[deleted]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/all_data_filtered_delauthor.pkl', 'wb') as f:\n",
    "    pickle.dump(all_data_filtered, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords for lexical search from Eschliman et al. (2024)\n",
    "keywords = ['discrimination', 'discriminate', 'judge', 'judgment', 'judgement', 'other', 'different', \n",
    "            'not made for', 'label', 'labeling', 'labelling', 'stereotype', 'stereotyping', \n",
    "            'stereotypical', 'type', 'typed', 'status', 'shame', 'shamed', 'power', 'reputation',\n",
    "              'rep', 'rap', 'disregard', 'junkie', 'addict', 'criminal', 'hate', 'hater']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discrimination\n",
      "discriminate\n",
      "judge\n",
      "judgment\n",
      "judgement\n",
      "other\n",
      "different\n",
      "not made for\n",
      "label\n",
      "labeling\n",
      "labelling\n",
      "stereotype\n",
      "stereotyping\n",
      "stereotypical\n",
      "type\n",
      "typed\n",
      "status\n",
      "shame\n",
      "shamed\n",
      "power\n",
      "reputation\n",
      "rep\n",
      "rap\n",
      "disregard\n",
      "junkie\n",
      "addict\n",
      "criminal\n",
      "hate\n",
      "hater\n"
     ]
    }
   ],
   "source": [
    "# max 25 posts\n",
    "# search for keywords in the selftext\n",
    "sample_posts = []\n",
    "for keyword in keywords:\n",
    "    print(keyword)\n",
    "    sample_posts.append(all_data[all_data['selftext'].str.contains(keyword, case=False)].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Long-shot trial with cosine similarity..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 29 posts in sample\n",
    "# will review manuall then do semantic search\n",
    "\n",
    "sample_posts_df = pd.concat(sample_posts, axis=0)\n",
    "sample_posts_df.to_csv('../data/sample_posts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example post with enacted stigma:\n",
    "example_post = '''\n",
    "So I have a long history with psychiatric medications, when I was younger my parents had me on Ritalin for a year or so, and then right about when I was 18 I got severely depressed for the first time. My mom suggested I see a psychiatrist. I did and was put on an antidepressant and it helped tremendously.\n",
    "\n",
    "Throughout my entire life I‚Äôve had emotional issues. I have consistently had sleeping issues since I have been extremely young, my anxiety has always been ridiculously high leading to many embarrassing situations for myself and a lot of missed opportunities, and thinking back on everything I have noticed my capacity to obsess. When I was young I was consistently obsessed with something. Obsessed to the point of, when I wanted to get my ears pierced I decided to staple my ears.\n",
    "\n",
    "Fast forward, I am 28-years-old. I now currently taking antidepressant, and a mood stabilizer for a diagnosed bipolar disorder. Over the past six years I have been on and off for benzodiazepines, and now currently take 4 mg a day of Clonopin. I realize this is a fairly high dose, some days I take three, however the pain of benzodiazepine withdrawal has never been severe enough for me compared to the amount of panic attacks and anxiety I have without that.\n",
    "\n",
    "I also take a blood pressure medication for hypertension.\n",
    "\n",
    "As for sleep, right now I am taking sonata. I feel that it works fairly well however, I struggle still, with an improved roughly 5 hours of sleep vs 3 a night. My heart rate is consistently above 90 BPM, and I still suffer from panic attacks and anxiety.\n",
    "\n",
    "I realize my medication regiment is high however, I believe I am an adult and I have the right to choose what goes in to my bod, especially with a supportive psychiatrist on my side.\n",
    "\n",
    "Lately, the pharmacy has been extremely disrespectful to me due to the medications I take. Lately, I tried Ambien CR, which worked miserably, and I wanted to switch back to my sonata. The pharmacist would not let me until the Ambien script was up due to the fact that they were in the same class of drugs. I felt very discriminated against and I thought it was quite disrespectful.\n",
    "\n",
    "My parents also consistently questioning the medications I take and in a given week probably ask me‚ÄúDo you really need to take all of those medications?‚Äù least three or four times.\n",
    "\n",
    "I am having a bit of a psychological dilemma. I am consistently in fear that the pharmacy will find a way to not give me my medication and that my parents and girlfriend will continue to blame the majority of my ‚Äúissues‚Äù on these medications.\n",
    "\n",
    "The reason that I am writing this because I wanted to get others opinions on how I should deal with my significant others, and if I really should possibly work hard to get off of these medications. I don‚Äôt know what to do and I am a little bit terrified. \n",
    "\n",
    "Last month there was a day where I was unable to use my clonazepam and I really let it get to me. I don‚Äôt like where this is gotten also- however I have been on and off of these, and realize that when I get off of them things will be worse even with all of the exercise, mindfulness, meditation, and yoga combined with a healthy diet and positive mental reinforcement. \n",
    "\n",
    "I‚Äôve tried it all folks and I just don‚Äôt know what to do.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "all_data['text'] = all_data['title'] + ' ' + all_data['selftext']\n",
    "\n",
    "# minimal preprocessing\n",
    "def preprocess_text(text):\n",
    "   # correct misencoded characters\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')  \n",
    "   # Normalize text to lower case\n",
    "    text = text.lower()\n",
    "    # Remove numbers and punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    # Tokenize text\n",
    "    words = text.split()\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # remove whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorization\n",
    "vectorizer = TfidfVectorizer(preprocessor= preprocess_text)\n",
    "X = vectorizer.fit_transform(all_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity with the example post\n",
    "example_vector = vectorizer.transform([example_post])\n",
    "cosine_similarities = cosine_similarity(example_vector, X)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter and collect posts with >= 80% similarity\n",
    "threshold = 0.8  # 80% similarity\n",
    "similar_posts_indices = np.where(cosine_similarities >= threshold)[0]\n",
    "similar_posts = [all_data[i] for i in similar_posts_indices]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
