{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "dat = pd.read_csv('../data/destigma_pipeline/task2/stigma-posts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stigma by type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label types of stigma types\n",
    "# keywords\n",
    "self = ['self-stigma', 'internalization', 'Self-Stigma', 'self-stigmatizing']\n",
    "structural = ['structural stigma', 'institutional stigma', 'systemic stigma']\n",
    "\n",
    "# regex\n",
    "self_re = re.compile(r'\\bself-stigma\\b|\\binternalization\\b|\\bSelf-Stigma\\b|\\bself-stigmatizing\\b')\n",
    "structural_re = re.compile(r'\\bstructural stigma\\b|\\binstitutional stigma\\b|\\bsystemic stigma\\b')\n",
    "\n",
    "# create new columns\n",
    "dat['stigma_type'] = np.nan\n",
    "dat['stigma_type'] = np.where(dat['stigmaExplanation'].str.contains(self_re), 'self-stigma', dat['stigma_type'])\n",
    "dat['stigma_type'] = np.where(dat['stigmaExplanation'].str.contains(structural_re), 'structural stigma', dat['stigma_type'])\n",
    "# make np.nan to 'directed stigma'\n",
    "dat['stigma_type'] = np.where(dat['stigma_type']=='nan', 'directed stigma', dat['stigma_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "directed stigma      1949\n",
       "self-stigma          1199\n",
       "structural stigma      59\n",
       "Name: stigma_type, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat['stigma_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually correct some of the labels\n",
    "# ids= ['ej9n8h', 'e0f34z', 'aopd9s', 'iijeac', 'xz3ah2', 'aasrzu', '3gj2ac', '4mdyst', '41v8m5', '1t0y0y', 'ez09dy', 'uvwabs', '9sjnir', 'awmgr2', 'zr34f0', '6eie6n', '4zwshz', 'nocycv', 'ylife', 'mofShe', 'Swiw0z', 'aOylmk']\n",
    "# dat_subs = dat[dat['id'].isin(ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stigma by substance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "narcotics = ['narcotics', 'opioids', 'heroin', 'fentanyl', 'opiates', 'opiate', 'opioid', 'opiods', 'oxy', 'oxycodone', 'hydrocodone', 'codeine', 'morphine', 'methadone', 'buprenorphine', 'suboxone', 'subutex',\n",
    "             'dilaudid', 'vicodin', 'percocet', 'percocets', 'roxicodone', 'roxy', 'roxies', 'oxymorphone', 'oxymorphones', \n",
    "             'hydromorphone', 'hydromorphones', 'pain pills', 'opium', 'opoid','fent', 'dope','painkillers', 'pain killers']\n",
    "hallucinogens = ['hallucinogens', 'lsd', 'acid', 'mushrooms', 'psilocybin', 'dmt', 'ayahuasca', 'peyote', 'mdma', 'ecstasy', 'molly','ketamine','salvia', \n",
    "                 'dissociatives', 'pcp', 'angel dust', 'shrooms']\n",
    "depressants = ['depressants', 'benzos', 'xanax', 'valium', 'ativan', 'klonopin', 'librium', 'tranquilizers', 'barbiturates', 'sleeping pills', 'sedatives',\n",
    "               'benzodiazepines', 'benzodiazepine', 'sedative']\n",
    "stims = ['stimulants', 'meth', 'methamphetamine', 'cocaine', 'crystal meth', 'speed', 'adderall', 'ritalin', 'amphetamine', 'amphetamines',\n",
    "         'blow', 'coke', 'crack', 'crack cocaine', 'crack-cocaine']\n",
    "drugs_of_concern = ['dxm', 'dextromethorphan', 'kratom', 'fake pills']\n",
    "designer_drugs = ['bath salts', 'flakka','gravel', 'cloud nine', 'snow blow', 'vanilla sky', 'lunar wave', 'white lightning']\n",
    "cannabis = ['cannabis', 'weed', 'marijuana', 'pot', 'thc', 'cbd']\n",
    "synthetic_cannabinoids = ['k2', 'spice', 'synthetic cannabinoids', 'synthetic marijuana']\n",
    "reversal_agents = ['naloxone', 'narcan', 'nalmefene', 'kloxxado', 'evzio', 'naloxone hydrochloride', 'naloxone hcl']\n",
    "other = ['inhalants', 'steroids', 'jenkem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if any of the keywords are in the language posts\n",
    "dat['substance_type'] = np.nan\n",
    "dat['substances'] = np.nan\n",
    "dat['text'] = dat['text'].str.lower()\n",
    "\n",
    "for index, row in dat.iterrows():\n",
    "    substance_types = []\n",
    "    substances = []\n",
    "    for drug in narcotics:\n",
    "        if drug in row['text']:\n",
    "            substance_types.append('narcotics')\n",
    "            substances.append(drug)\n",
    "    for drug in hallucinogens:\n",
    "        if drug in row['text']:\n",
    "            substance_types.append('hallucinogens')\n",
    "            substances.append(drug)\n",
    "    for drug in depressants:\n",
    "        if drug in row['text']:\n",
    "            substance_types.append('depressants')\n",
    "            substances.append(drug)\n",
    "    for drug in stims:\n",
    "        if drug in row['text']:\n",
    "            substance_types.append('stimulants')\n",
    "            substances.append(drug)\n",
    "    for drug in drugs_of_concern:\n",
    "        if drug in row['text']:\n",
    "            substance_types.append('drugs_of_concern')\n",
    "            substances.append(drug)\n",
    "    for drug in designer_drugs:\n",
    "        if drug in row['text']:\n",
    "            substance_types.append('designer_drugs')\n",
    "            substances.append(drug)\n",
    "    for drug in cannabis:\n",
    "        if drug in row['text']:\n",
    "            substance_types.append('cannabis')\n",
    "            substances.append(drug)\n",
    "    for drug in synthetic_cannabinoids:\n",
    "        if drug in row['text']:\n",
    "            substance_types.append('synthetic_cannabinoids')\n",
    "            substances.append(drug)\n",
    "    for drug in other:\n",
    "        if drug in row['text']:\n",
    "            substance_types.append('other')\n",
    "            substances.append(drug)\n",
    "    for drug in reversal_agents:\n",
    "        if drug in row['text']:\n",
    "            substance_types.append('reversal_agents')\n",
    "            substances.append(drug)\n",
    "    if len(substance_types) == 0:\n",
    "        substance_types.append('unspecified')\n",
    "    dat.at[index, 'substance_type'] = ', '.join(set(substance_types))\n",
    "    dat.at[index, 'substances'] = ', '.join(set(substances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unspecified                                                       1021\n",
       "stimulants                                                         591\n",
       "cannabis                                                           379\n",
       "narcotics                                                          309\n",
       "stimulants, narcotics                                              200\n",
       "                                                                  ... \n",
       "narcotics, hallucinogens, stimulants, cannabis, designer_drugs       1\n",
       "stimulants, designer_drugs                                           1\n",
       "reversal_agents, cannabis                                            1\n",
       "narcotics, drugs_of_concern                                          1\n",
       "stimulants, narcotics, reversal_agents, depressants                  1\n",
       "Name: substance_type, Length: 66, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat['substance_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "narcotics: 769\n",
      "hallucinogens: 162\n",
      "depressants: 200\n",
      "stims: 1218\n",
      "drugs_of_concern: 14\n",
      "designer_drugs: 6\n",
      "cannabis: 818\n",
      "synthetic_cannabinoids: 14\n",
      "reversal_agents: 41\n",
      "other: 8\n",
      "unspecified: 1021\n"
     ]
    }
   ],
   "source": [
    "# count the number of stigma types in all posts\n",
    "narcotics = 0\n",
    "hallucinogens = 0\n",
    "depressants = 0\n",
    "stims = 0\n",
    "drugs_of_concern = 0\n",
    "designer_drugs = 0\n",
    "cannabis = 0\n",
    "synthetic_cannabinoids = 0\n",
    "reversal_agents = 0\n",
    "other = 0\n",
    "unspecified = 0\n",
    "for index, row in dat.iterrows():\n",
    "    if 'narcotics' in row['substance_type']:\n",
    "        narcotics += 1\n",
    "    if 'hallucinogens' in row['substance_type']:\n",
    "        hallucinogens += 1\n",
    "    if 'depressants' in row['substance_type']:\n",
    "        depressants += 1\n",
    "    if 'stimulants' in row['substance_type']:\n",
    "        stims += 1\n",
    "    if 'drugs_of_concern' in row['substance_type']:\n",
    "        drugs_of_concern += 1\n",
    "    if 'designer_drugs' in row['substance_type']:\n",
    "        designer_drugs += 1\n",
    "    if 'cannabis' in row['substance_type']:\n",
    "        cannabis += 1\n",
    "    if 'synthetic_cannabinoids' in row['substance_type']:\n",
    "        synthetic_cannabinoids += 1\n",
    "    if 'reversal_agents' in row['substance_type']:\n",
    "        reversal_agents += 1\n",
    "    if 'other' in row['substance_type']:\n",
    "        other += 1\n",
    "    if 'unspecified' in row['substance_type']:\n",
    "        unspecified += 1\n",
    "\n",
    "print('narcotics:', narcotics)\n",
    "print('hallucinogens:', hallucinogens)\n",
    "print('depressants:', depressants)\n",
    "print('stims:', stims)\n",
    "print('drugs_of_concern:', drugs_of_concern)\n",
    "print('designer_drugs:', designer_drugs)\n",
    "print('cannabis:', cannabis)\n",
    "print('synthetic_cannabinoids:', synthetic_cannabinoids)\n",
    "print('reversal_agents:', reversal_agents)\n",
    "print('other:', other)\n",
    "print('unspecified:', unspecified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stigma_type             directed stigma  self-stigma  structural stigma\n",
      "substance_type                                                         \n",
      "stimulants                          818          380                 20\n",
      "unspecified                         537          475                  9\n",
      "cannabis                            515          276                 27\n",
      "narcotics                           501          250                 18\n",
      "depressants                          92          102                  6\n",
      "hallucinogens                        90           68                  4\n",
      "reversal_agents                      38            3                  0\n",
      "drugs_of_concern                      7            7                  0\n",
      "synthetic_cannabinoids               11            3                  0\n",
      "other                                 4            3                  1\n",
      "designer_drugs                        6            0                  0\n"
     ]
    }
   ],
   "source": [
    "dat2 = dat.copy()\n",
    "dat2[\"substance_type\"] = dat2[\"substance_type\"].str.split(\", \")\n",
    "dat2 = dat2.explode(\"substance_type\")\n",
    "dat2[\"substance_type\"] = dat2[\"substance_type\"].str.strip()\n",
    "\n",
    "table = pd.crosstab(dat2['substance_type'], dat2['stigma_type'])\n",
    "subs_totals = table.sum(axis=1)\n",
    "stigma_totals = table.sum(axis=0)\n",
    "sorted_table = table.reindex(subs_totals.sort_values(ascending=False).index, axis=0)\n",
    "\n",
    "table.to_csv('../data/destigma_pipeline/task2/stigma-substance-table.csv')\n",
    "print(sorted_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "offmychest          2565\n",
       "unpopularopinion     460\n",
       "nursing              156\n",
       "medicine              26\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group by stigma type and subreddit and count the number of posts\n",
    "dat['subreddit'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.to_csv('../data/destigma_pipeline/task2/stigma-posts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phrase extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "directed = dat[dat['stigma_type']=='directed stigma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = dat[dat['stigma_type']=='self-stigma']\n",
    "structural = dat[dat['stigma_type']=='structural stigma']\n",
    "\n",
    "self.to_csv('../data/destigma_pipeline/task2/self-stigma-posts.csv', index=False)\n",
    "structural.to_csv('../data/destigma_pipeline/task2/structural-stigma-posts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "offmychest          1339\n",
       "unpopularopinion     445\n",
       "nursing              143\n",
       "medicine              22\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directed['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import spacy_transformers\n",
    "from collections import Counter\n",
    "\n",
    "# meaninful phrases\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "def extract_meaningful_phrases(text):\n",
    "    doc = nlp(text)\n",
    "    # Filter out pronouns and single-character words in noun chunks\n",
    "    return [chunk.text for chunk in doc.noun_chunks if chunk.root.pos_ not in ['PRON'] and len(chunk.text) > 1]\n",
    "\n",
    "# Apply the function to extract phrases\n",
    "directed['phrases'] = directed['text'].apply(extract_meaningful_phrases)\n",
    "phrases_flat = [phrase for sublist in directed['phrases'] for phrase in sublist]\n",
    "phrase_counts = Counter(phrases_flat)\n",
    "\n",
    "# Display the most common phrases\n",
    "print(phrase_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('one', 335), ('first', 241), ('my life', 186), ('my mom', 175), ('two', 175), ('a lot', 174), ('today', 155), ('my brother', 141), ('my family', 130), ('my dad', 117), ('no one', 116), ('my mother', 111), ('the time', 109), ('my parents', 102), ('a job', 99), ('the house', 99), ('my sister', 98), ('the way', 92), ('an addict', 91), ('your life', 90)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/bt0ykz9n5_16z2gvbybl0tbc0000gn/T/ipykernel_28010/1644033549.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  directed['phrases2'] = directed['text'].apply(extract_complex_phrases)\n"
     ]
    }
   ],
   "source": [
    "# refine noun extraction\n",
    "def extract_complex_phrases(text):\n",
    "    doc = nlp(text)\n",
    "    # Combine noun chunks and named entities for richer context\n",
    "    phrases = set(chunk.text for chunk in doc.noun_chunks if len(chunk.text.split()) > 1)\n",
    "    entities = set(ent.text for ent in doc.ents)\n",
    "    return list(phrases.union(entities))\n",
    "\n",
    "directed['phrases2'] = directed['text'].apply(extract_complex_phrases)\n",
    "phrases_flat2 = [phrase for sublist in directed['phrases2'] for phrase in sublist]\n",
    "phrase_counts2 = Counter(phrases_flat2)\n",
    "\n",
    "print(phrase_counts2.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('is is', 1705), ('was was', 1090), (\"'m i\", 987), (\"'s 's\", 809), (\"'s it\", 805), ('know i', 745), ('have i', 655), ('have have', 647), ('was i', 624), ('are are', 622), ('know know', 607), ('want i', 540), ('was it', 522), (\"'m 'm\", 470), ('had had', 462), ('am i', 426), ('feel i', 423), (\"'re you\", 423), ('had i', 412), ('was he', 402)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/bt0ykz9n5_16z2gvbybl0tbc0000gn/T/ipykernel_28010/2714676999.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  directed['phrases_dp'] = directed['text'].apply(extract_dependency_phrases)\n"
     ]
    }
   ],
   "source": [
    "# extract based on dependency parsing\n",
    "\n",
    "def extract_dependency_phrases(text):\n",
    "    doc = nlp(text)\n",
    "    phrases = []\n",
    "    for token in doc:\n",
    "        # Extract phrases based on syntactic dependencies\n",
    "        if token.dep_ in ('nsubj', 'dobj', 'pobj', 'attr', 'ROOT'):\n",
    "            phrases.append(token.head.text + ' ' + token.text)\n",
    "    return phrases\n",
    "\n",
    "directed['phrases_dp'] = directed['text'].apply(extract_dependency_phrases)\n",
    "phrases_flat_dp = [phrase for sublist in directed['phrases_dp'] for phrase in sublist]\n",
    "phrase_counts_dp = Counter(phrases_flat_dp)\n",
    "\n",
    "print(phrase_counts_dp.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 16933), ('you', 6257), ('he', 4593), ('she', 4128), ('it', 3426), ('they', 2437), ('we', 1861), ('that', 1554), ('who', 1233), ('this', 647), ('me', 595), ('her', 301), ('people', 300), ('\\\\n\\\\ni', 283), ('him', 265), ('what', 237), ('which', 234), ('my mom', 218), ('them', 157), ('my dad', 152)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/bt0ykz9n5_16z2gvbybl0tbc0000gn/T/ipykernel_28010/4063420858.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  directed['phrases_subtr'] = directed['text'].apply(extract_subtrees)\n"
     ]
    }
   ],
   "source": [
    "# extract subtree\n",
    "def extract_subtrees(text):\n",
    "    doc = nlp(text)\n",
    "    phrases = []\n",
    "    for token in doc:\n",
    "        if token.dep_ in ['nsubj', 'obj', 'iobj']:  # You can modify the list of dependency tags\n",
    "            # Join the tokens in the subtree into a single string\n",
    "            subtree = ' '.join([t.text for t in token.subtree])\n",
    "            phrases.append(subtree)\n",
    "    return phrases\n",
    "\n",
    "# Apply the function to extract phrases\n",
    "directed['phrases_subtr'] = directed['text'].apply(extract_subtrees)\n",
    "phrases_flat_s = [phrase for sublist in directed['phrases_subtr'] for phrase in sublist]\n",
    "phrase_counts_s = Counter(phrases_flat_s)\n",
    "\n",
    "# Display the most common phrases\n",
    "print(phrase_counts_s.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('n', 3203), ('’', 2931), ('know', 1051), ('get', 1017), ('want', 781), ('drugs', 702), ('life', 668), ('people', 641), ('going', 613), ('fuck', 581), ('time', 562), ('.\\\\ n', 558), ('like', 516), ('ni', 501), ('think', 500), ('go', 469), ('shit', 458), ('one', 456), ('see', 429), ('family', 422)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/bt0ykz9n5_16z2gvbybl0tbc0000gn/T/ipykernel_28010/4260552157.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  directed['keywords'] = directed['text'].apply(extract_keywords)\n"
     ]
    }
   ],
   "source": [
    "from rake_nltk import Rake \n",
    "\n",
    "rake = Rake()\n",
    "\n",
    "def extract_keywords(text):\n",
    "    rake.extract_keywords_from_text(text)\n",
    "    return rake.get_ranked_phrases()\n",
    "\n",
    "directed['keywords_rake'] = directed['text'].apply(extract_keywords)\n",
    "keywords_flat = [keyword for sublist in directed['keywords_rake'] for keyword in sublist]\n",
    "keyword_counts = Counter(keywords_flat)\n",
    "\n",
    "print(keyword_counts.most_common(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/laylabouzoubaa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/laylabouzoubaa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('people', 317), ('drugs', 281), ('drug', 248), ('time', 218), ('fucking', 211), ('n’t', 209), ('life', 202), ('years', 163), ('fuck', 149), ('weed', 149), ('shit', 136), ('mom', 130), ('feel', 129), ('hate', 127), ('friend', 121), ('addict', 120), ('friends', 118), ('brother', 114), ('family', 109), ('back', 109)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/bt0ykz9n5_16z2gvbybl0tbc0000gn/T/ipykernel_28010/445724127.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  directed['keywords_yake'] = directed['text'].apply(extract_keywords_yake)\n"
     ]
    }
   ],
   "source": [
    "import yake\n",
    "\n",
    "yake_kw = yake.KeywordExtractor(lan='en', n=4, dedupLim=0.9, dedupFunc='seqm', windowsSize=1, top=20, features=None)\n",
    "\n",
    "def extract_keywords_yake(text):\n",
    "    keywords = yake_kw.extract_keywords(text)\n",
    "    return [keyword[0] for keyword in keywords]\n",
    "\n",
    "directed['keywords_yake'] = directed['text'].apply(extract_keywords_yake)\n",
    "keywords_flat_yake = [keyword for sublist in directed['keywords_yake'] for keyword in sublist]\n",
    "keyword_counts_yake = Counter(keywords_flat_yake)\n",
    "\n",
    "print(keyword_counts_yake.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done for  addict\n",
      "done for  drugs\n",
      "done for  junkie\n",
      "done for  overdose\n",
      "done for  clean\n",
      "done for  rehab\n"
     ]
    }
   ],
   "source": [
    "seed_words = ['addict', 'drugs', 'junkie', 'overdose', 'clean', 'rehab']\n",
    "\n",
    "dict_of_phrases = {}\n",
    "\n",
    "for word in seed_words:\n",
    "    seed_posts = []\n",
    "    for index, row in directed.iterrows():\n",
    "        if word in row['text']:\n",
    "            seed_posts.append(row['text'])\n",
    "    \n",
    "    ngram_max = 4\n",
    "    numOfKeywords = 20\n",
    "    dedupFunc='seqm'\n",
    "    dedupLim=0.1\n",
    "    windowSize=1\n",
    "\n",
    "    kw_extractor = yake.KeywordExtractor(lan='en', \n",
    "                                         n=ngram_max, \n",
    "                                         dedupLim=dedupLim, \n",
    "                                         dedupFunc=dedupFunc, \n",
    "                                         windowsSize=windowSize, \n",
    "                                         top=numOfKeywords, features=None)\n",
    "    \n",
    "    keywords = kw_extractor.extract_keywords(' '.join(seed_posts))\n",
    "    dict_of_phrases[word] = [keyword[0] for keyword in keywords]\n",
    "\n",
    "    print(\"done for \", word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "addict ['sister that i ’ve', 'rehab a few weeks', 'fuck up. i thought', 'n’t', 'day', 'mom', 'ago', 'end', 'run', '’ll', 'idk', 'wtf', 'dog witnessed a drug', 'harrassing my family', 'leg', '5-10', 'gpa', 'yrs', 'completely psychotic', 'xxx']\n",
      "drugs ['mom will never find', 'shit took our daughter', 'weeks later she calls', 'n’t', 'back', 'guy', 'kids', 'job', 'hell drug addicts put', '’re', 'sex', 'dumbest things people', 'cpr', 'adhd', '5-10', 'usa', 'fbi', 'hmmm', '24-7', 'aaaaaaaaaaaaaaaaaaaaaaa']\n",
      "junkie ['n’t had a job', 'junkie', 'drugs', 'year', 'mom', 'living', 'call', 'piece of shit', 'good people', 'etc.', 'addict today', '5-10', 'flew', 'bbq', 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaahh', 'ffffuuuuuccckk', 'ahhhhh', 'assessments', '04:52']\n",
      "overdose ['made it in time', 'drug', 'years', 'high', 'good', 'fuck', 'knew', 'completely', 'nwhy', '5-10', 'bff', '24-7', 'initial labs are afu']\n",
      "clean ['makes them a lot', 'girl i was friends', 'drug', 'back', 'n’t', 'high', 'job', 'full', '’ve', 'cps', 'odd', '12-13', 'sky', 'per.', '9:30', 'gym', 'absolutely convinced', 'attending university']\n",
      "rehab ['call from my mother', 'person i should care', 'brother is a fucking', 'dad was getting back', 'drugs', 'n’t', 'job', 'bad', 'girl', '’ve', 'sex', 'cna', 'flip', 'off.', 'time the nurse sends', 'unhappy all the time', 'aww', '15-17', '9:30']\n"
     ]
    }
   ],
   "source": [
    "for key, value in dict_of_phrases.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export dict\n",
    "dict_df = pd.DataFrame.from_dict(dict_of_phrases, orient='index')\n",
    "dict_df.to_csv('../data/destigma_pipeline/task2/stigma_posts_phrases.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "directed.to_csv('../data/destigma_pipeline/task2/directed_stigma.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Extracting negative verbs and negative associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/bt0ykz9n5_16z2gvbybl0tbc0000gn/T/ipykernel_53555/348038420.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  directed['sentiment'] = directed['text'].apply(lambda x: sia.polarity_scores(x)['compound'])\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'text' column in your DataFrame contains the posts\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "directed['sentiment'] = directed['text'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "\n",
    "# Filter out sentences with negative sentiment\n",
    "negative_posts = directed[directed['sentiment'] < 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name util",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# extract verbs\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m      3\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_verbs\u001b[39m(text):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/__init__.py:14\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prefer_gpu, require_gpu, require_cpu  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcli\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m info  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglossary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m explain  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/pipeline/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattributeruler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AttributeRuler\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdep_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DependencyParser\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01medit_tree_lemmatizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EditTreeLemmatizer\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/pipeline/attributeruler.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msrsly\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipe\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Example\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/pipeline/pipe.pyx:1\u001b[0m, in \u001b[0;36minit spacy.pipeline.pipe\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/vocab.pyx:1\u001b[0m, in \u001b[0;36minit spacy.vocab\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/tokens/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Doc\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtoken\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Token\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspan\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Span\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/tokens/doc.pyx:34\u001b[0m, in \u001b[0;36minit spacy.tokens.doc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name util"
     ]
    }
   ],
   "source": [
    "# extract verbs\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_verbs(text):\n",
    "    doc = nlp(text)\n",
    "    verbs = [token.lemma_ for token in doc if token.pos_ == 'VERB']\n",
    "    return verbs\n",
    "\n",
    "negative_posts['verbs'] = negative_posts['text'].apply(extract_verbs)\n",
    "verb_counts = negative_posts['verbs'].explode().value_counts()\n",
    "print(verb_counts.head(20))  # Print top 20 verbs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## semantic role labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.tree import Tree\n",
    "\n",
    "def extract_np_vp_chunks(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    pos_tags = pos_tag(words)\n",
    "\n",
    "    # Chunk sentence into noun phrases (NP) and verb phrases (VP) using a basic grammar\n",
    "    grammar = r\"\"\"\n",
    "        NP: {<DT|PP\\$>?<JJ>*<NN.*>+}  # Chunk sequences of DT, JJ, NN\n",
    "        VP: {<VB.*><NP|PP|CLAUSE>+$}  # Chunk verbs and their arguments\n",
    "        PP: {<IN><NP>}                # Chunk prepositions followed by NP\n",
    "        CLAUSE: {<NP><VP>}            # Chunk NP, VP\n",
    "    \"\"\"\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    tree = cp.parse(pos_tags)\n",
    "    \n",
    "    return tree\n",
    "\n",
    "def simple_srl(sentence):\n",
    "    tree = extract_np_vp_chunks(sentence)\n",
    "    semantic_roles = []\n",
    "    \n",
    "    # Let's define ARG0 typically as the subject NP, ARG1 as the object NP, and V as the main verb\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'VP':\n",
    "            verb = None\n",
    "            args = []\n",
    "            for s in subtree:\n",
    "                if type(s) is Tree:\n",
    "                    if s.label() == 'NP':\n",
    "                        args.append(' '.join(word for word, tag in s.leaves()))\n",
    "                else:\n",
    "                    verb = s[0]\n",
    "            if verb:\n",
    "                semantic_roles.append(('V', verb))\n",
    "                if args:\n",
    "                    semantic_roles.append(('ARG0', args[0]))  # Simplistically considering the first NP as ARG0\n",
    "                    if len(args) > 1:\n",
    "                        semantic_roles.append(('ARG1', args[1]))  # And the second NP as ARG1\n",
    "\n",
    "    return semantic_roles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/bt0ykz9n5_16z2gvbybl0tbc0000gn/T/ipykernel_40057/2383323980.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  directed['srl'] = directed['text'].apply(simple_srl)\n"
     ]
    }
   ],
   "source": [
    "# apply srl to the text\n",
    "directed['srl'] = directed['text'].apply(simple_srl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "directed_srl = directed[directed['srl'].apply(lambda x: len(x) > 0)][['text', 'srl']]\n",
    "directed_srl.to_csv('../data/destigma_pipeline/task2/stigma-posts-srl.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "def analyze_sentence(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    semantic_roles = []\n",
    "\n",
    "    # Extract entities and their roles based on dependency parsing\n",
    "    for token in doc:\n",
    "        # Finding verb (predicate)\n",
    "        if token.pos_ == 'VERB':\n",
    "            subject = ''\n",
    "            object = ''\n",
    "            # Searching for subject and object relations\n",
    "            for child in token.children:\n",
    "                if child.dep_ in ('nsubj', 'nsubjpass'):\n",
    "                    subject = child.text\n",
    "                elif child.dep_ in ('dobj', 'attr', 'prep', 'ccomp'):\n",
    "                    object = child.text\n",
    "            if subject:\n",
    "                semantic_roles.append(('V', token.text, 'ARG0', subject))\n",
    "            if object:\n",
    "                semantic_roles.append(('V', token.text, 'ARG1', object))\n",
    "    \n",
    "    return semantic_roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "directed_srl = pd.read_csv('../data/destigma_pipeline/task2/directed_srl.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phrase extraction \n",
    "from collections import Counter\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# define phrases\n",
    "phrases = ['drug addict', 'drug addiction', 'addict', 'addiction', 'junkie', \n",
    "              'druggie', 'druggies', 'addicts', 'od', 'overdose', 'low-life']\n",
    "\n",
    "# add phrases to matcher\n",
    "patterns = [nlp.make_doc(text) for text in phrases]\n",
    "matcher.add('stigma', None, *patterns)\n",
    "\n",
    "# extract phrases\n",
    "phrases = []\n",
    "for index, row in directed.iterrows():\n",
    "    doc = nlp(row['text'])\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        phrases.append(span.text)\n",
    "\n",
    "# count phrases\n",
    "phrase_counts = Counter(phrases)\n",
    "print(phrase_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 1274 matches:\n",
      "ruggy . you can tell my sister is on drugs because she doesnt shut her yap and \n",
      "d my mom says my sister is n't doing drugs but she ca n't tell.\\n\\nno wonder i \n",
      "sister is a liar , steals , and does drugs . she tries to make you feel like yo\n",
      "rve major kudos . hey i 've done the drugs , i did coke like nobodys business i\n",
      "ast 13 years and now she wants to do drugs again .... drugs suck i do n't care \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.text import Text\n",
    "\n",
    "# Tokenize the complete text corpus\n",
    "tokens = [word for sent in directed['text'] for word in nltk.word_tokenize(sent)]\n",
    "text_obj = Text(tokens)\n",
    "\n",
    "# Function to print concordances of phrases\n",
    "def print_concordances(phrase, width=80):\n",
    "    text_obj.concordance(phrase, width=width, lines=5)\n",
    "\n",
    "# Example usage for the phrase 'substance use'\n",
    "print_concordances('drugs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors import Predictor\n",
    "import allennlp_models.tagging\n",
    "\n",
    "# Load the pre-trained Semantic Role Labeling model\n",
    "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/bert-base-srl-2020.11.19.tar.gz\")\n",
    "\n",
    "# Define a function to apply SRL to a sentence\n",
    "def semantic_role_labeling(text):\n",
    "    # Use the predictor on the provided text\n",
    "    results = predictor.predict(sentence=text)\n",
    "    \n",
    "    # Extract and return the verbs and their roles\n",
    "    verbs = results['verbs']\n",
    "    formatted_results = []\n",
    "    for verb in verbs:\n",
    "        description = verb['description']\n",
    "        formatted_results.append(description)\n",
    "    \n",
    "    return formatted_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
